# -*- coding: utf-8 -*-
"""Random_forest_regressor

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1whRYjVUNQwypBqhMkZcPXIp2_ybUZmuo
"""

#Importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

dataset = pd.read_csv('Housing DB.csv')
x = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

#Training and Testing the dataset
from sklearn.model_selection import train_test_split
x_train, x_test,y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

print(x_train)

print(y_train)

print(x_test)

print(y_test)

from sklearn.preprocessing import StandardScaler
sc =  StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

#RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(
    n_estimators=500,
    max_depth=15,
    min_samples_split=2,
    min_samples_leaf=1,
    random_state=42
)

print(x_train)

print(x_test)

def model_complexity(model):
    from sklearn.ensemble import RandomForestClassifier
    if isinstance(model, RandomForestClassifier):
        depths = [est.get_depth() for est in model.estimators_]
        return f"{model.n_estimators} trees, avg depth = {np.mean(depths):.1f}"

from sklearn.ensemble import RandomForestRegressor

regressor = RandomForestRegressor(n_estimators=100, random_state=0)
regressor.fit(x_train, y_train)

# Predictions
y_pred = regressor.predict(x_test)

# Evaluation
from sklearn.metrics import mean_absolute_error, r2_score
print("MAE:", mean_absolute_error(y_test, y_pred))
print("RÂ² Score:", r2_score(y_test, y_pred))

# The scaler was fitted on 13 features, so we need to provide 13 features for prediction
mean_features = x_train.mean(axis=0)
input_features = np.array([[21.2, 1000] + mean_features[2:].tolist()])
print(regressor.predict(sc.transform(input_features)))

y_pred = regressor.predict(x_test)
print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test),1)), 1))

#Predictions
# Find the median value and 3 samples closest to the median
median = dataset['MEDV'].median()
closest_idx = np.abs(dataset['MEDV'] - median).argsort()[:3]
median_samples = dataset.iloc[closest_idx]

# Drop the target column to get only features
X_median3 = median_samples.drop(columns=['MEDV'])

# Scale with the same scaler used in training (assuming sc as scaler)
X_median3_scaled = sc.transform(X_median3.values)

# Predict with your trained regressor
predictions = regressor.predict(X_median3_scaled)

print("Input features for 3 median-price samples:")
print(X_median3)
print("\nPredictions for these samples:")
print(predictions)

# ---- Create side-by-side plots ----
fig, axes = plt.subplots(1, 2, figsize=(14,6))

# Left: Predictions vs Actuals
axes[0].bar(range(len(sample_preds)), sample_actuals, width=0.4, label="Actual", align="edge")
axes[0].bar([i+0.4 for i in range(len(sample_preds))], sample_preds, width=0.4, label="Predicted", align="edge")
axes[0].set_xticks([0.2, 1.2, 2.2])
axes[0].set_xticklabels([f"Sample {i}" for i in sample_indices])
axes[0].set_ylabel("Median Price")
axes[0].set_title("Predicted vs Actual Values (3 Samples)")
axes[0].legend()

# Right: Feature Importances
sns.barplot(x=feat_importances.values, y=feat_importances.index, ax=axes[1], palette="viridis")
axes[1].set_title("Feature Importances from Random Forest")
axes[1].set_xlabel("Importance Score")
axes[1].set_ylabel("Features")

plt.tight_layout()
plt.show()
